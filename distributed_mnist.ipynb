{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "98fb0241-aae8-4b50-a481-dd9be23adefd",
   "metadata": {},
   "source": [
    "# Distributed training with Keras example using MNIST dataset\n",
    "\n",
    "This tutorial is adapted from the TensorFlow tutorial here: [https://www.tensorflow.org/tutorials/distribute/keras](https://www.tensorflow.org/tutorials/distribute/keras)\n",
    "\n",
    "To run this on HiPerGator, I requested an [Open onDemand](https://ood.rc.ufl.edu/) Jupyter session with:\n",
    "* 4 cores\n",
    "* 64 GB RAM\n",
    "* 2 A100 GPUs: `gpus:a100:2`\n",
    "\n",
    "This tutorial is intended to help get you up and running training models on HiPerGator and to show how simple it can be to scale things up as you exceed the capabilities of a single GPU. This tutorial also introduced [TensorBoard](https://www.tensorflow.org/tensorboard/get_started#:~:text=TensorBoard%20is%20a%20tool%20for,dimensional%20space%2C%20and%20much%20more.), a relatively easy method of tracking model training and your hyperparameter tuning experiments.\n",
    "\n",
    "## Load some modules and check TensorFlow version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "bbfe09e8-1319-435a-a642-d935b8ac6e9f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import tensorflow_datasets as tfds\n",
    "import tensorflow as tf\n",
    "\n",
    "import os\n",
    "\n",
    "# Load the TensorBoard notebook extension.\n",
    "%load_ext tensorboard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "db3dc02d-6c39-47f4-b409-23892ed1f564",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.7.0\n"
     ]
    }
   ],
   "source": [
    "print(tf.__version__)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "372df7f4-a211-45f6-a631-67fc409434da",
   "metadata": {},
   "source": [
    "## Load the MNIST dataset\n",
    "\n",
    "Remember the [MNIST dataset](https://en.wikipedia.org/wiki/MNIST_database) is the handwritten image dataset that was featured in the NVIDIA Foundations of Deep Learning (or Getting Started with Deep Learning) course.\n",
    "\n",
    "![Sample images from the MNIST dataset, Image by \n",
    "Suvanjanprasai from Wikipedia](images/MnistExamplesModified.png)\n",
    "\n",
    "We don't really need multiple GPUs to analyze these data, but they are handy to use and a familiar example."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "796707cc-421e-4ea0-a418-09094444be29",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-08-27 19:27:46.354924: I tensorflow/core/platform/cpu_feature_guard.cc:151] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  SSE4.1 SSE4.2 AVX AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2023-08-27 19:27:47.409554: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1525] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 79111 MB memory:  -> device: 0, name: NVIDIA A100-SXM4-80GB, pci bus id: 0000:b7:00.0, compute capability: 8.0\n",
      "2023-08-27 19:27:47.412858: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1525] Created device /job:localhost/replica:0/task:0/device:GPU:1 with 79111 MB memory:  -> device: 1, name: NVIDIA A100-SXM4-80GB, pci bus id: 0000:bd:00.0, compute capability: 8.0\n"
     ]
    }
   ],
   "source": [
    "datasets, info = tfds.load(name='mnist', with_info=True, as_supervised=True)\n",
    "\n",
    "mnist_train, mnist_test = datasets['train'], datasets['test']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "faa5c2d3-2e5c-41ab-b996-a7575279c992",
   "metadata": {},
   "source": [
    "## For multi-GPUs, setup a strategy\n",
    "\n",
    "There are many different ways of using multiple GPUs in training a model. As with many things, without a framework, things can get complex quickly. Luckily Keras provides a nice API framework to do most of the work for us.\n",
    "\n",
    "First, there are two main *things* that can be distributed across multiple GPUs:\n",
    "* **Data parallelism** uses multiple GPUs to train a single model with each GPU evaluating different batches of the dataset and averaging results periodically.\n",
    "* **Model parallelism** where a model is split across multiple GPUs. Models can be split with different layers on different GPUs, splitting the weights of a single layer across GPUs, or both.\n",
    "\n",
    "As you scale beyond a single GPU, it is easier (and more efficient) to use multiple GPUs in a single server (8 in the DGX Servers), but after that, you need to use GPUs on multiple servers (hosts).\n",
    "\n",
    "See more details [here](https://www.tensorflow.org/guide/keras/distributed_training).\n",
    "\n",
    "The cool thing is, that setting up multi-GPU training, can be quite simple. In this case, we really only need a couple of lines:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "47b1d827-1a3f-498a-98fa-a1f877b2f45a",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Using MirroredStrategy with devices ('/job:localhost/replica:0/task:0/device:GPU:0', '/job:localhost/replica:0/task:0/device:GPU:1')\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Using MirroredStrategy with devices ('/job:localhost/replica:0/task:0/device:GPU:0', '/job:localhost/replica:0/task:0/device:GPU:1')\n"
     ]
    }
   ],
   "source": [
    "# The MirroredStrategy works for multiple GPUs on 1 server.\n",
    "strategy = tf.distribute.MirroredStrategy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "37cb1413-8ae1-4669-9382-7793dfcd7be7",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of devices: 2\n"
     ]
    }
   ],
   "source": [
    "# Double check how many GPUs our strategy sees\n",
    "print('Number of devices: {}'.format(strategy.num_replicas_in_sync))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee294493-3948-46e1-89e1-036d0064bf41",
   "metadata": {},
   "source": [
    "## Set up the input pipeline\n",
    "\n",
    "As noted in the [original tutorial](https://www.tensorflow.org/tutorials/distribute/keras#set_up_the_input_pipeline), we can increase the batch size:\n",
    "> When training a model with multiple GPUs, you can use the extra computing power effectively by increasing the batch size. In general, use the largest batch size that fits the GPU memory and tune the learning rate accordingly.\n",
    "\n",
    "My guess is that on the A100s, we can go larger than 64, but let's start there."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "398ace1c-f9b1-4e21-99ba-9a105417b520",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# You can also do info.splits.total_num_examples to get the total\n",
    "# number of examples in the dataset.\n",
    "\n",
    "num_train_examples = info.splits['train'].num_examples\n",
    "num_test_examples = info.splits['test'].num_examples\n",
    "\n",
    "BUFFER_SIZE = 10000\n",
    "\n",
    "BATCH_SIZE_PER_REPLICA = 64\n",
    "BATCH_SIZE = BATCH_SIZE_PER_REPLICA * strategy.num_replicas_in_sync"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "46d4417d-be93-48a0-9f23-ad1091e3ce18",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def scale(image, label):\n",
    "   '''Define a function that normalizes the image pixel values from the [0, 255] range to the [0, 1] range'''\n",
    "   image = tf.cast(image, tf.float32)\n",
    "   image /= 255\n",
    "\n",
    "   return image, label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "830d67f4-0b24-4355-9a50-e23751a055a7",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#Apply this scale function to the training and test data, \n",
    "# and then use the tf.data.Dataset APIs to shuffle the training \n",
    "# data (Dataset.shuffle), and batch it (Dataset.batch). \n",
    "# Notice that you are also keeping an in-memory cache of the training \n",
    "# data to improve performance (Dataset.cache).\n",
    "\n",
    "train_dataset = mnist_train.map(scale).cache().shuffle(BUFFER_SIZE).batch(BATCH_SIZE)\n",
    "eval_dataset = mnist_test.map(scale).batch(BATCH_SIZE)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ea932d6-eefd-43a8-a65c-47cc0c69c6ed",
   "metadata": {},
   "source": [
    "## Create and compile the model\n",
    "\n",
    "The main change here is that we make the model within the context of the `Strategy.scope`, but making and compiling the model is the same as it would be on a single GPU."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "a8a051f3-9e3e-474e-8769-17d4c7933ba0",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n"
     ]
    }
   ],
   "source": [
    "with strategy.scope():\n",
    "  model = tf.keras.Sequential([\n",
    "      tf.keras.layers.Conv2D(32, 3, activation='relu', input_shape=(28, 28, 1)),\n",
    "      tf.keras.layers.MaxPooling2D(),\n",
    "      tf.keras.layers.Flatten(),\n",
    "      tf.keras.layers.Dense(64, activation='relu'),\n",
    "      tf.keras.layers.Dense(10)\n",
    "  ])\n",
    "\n",
    "  model.compile(loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n",
    "                optimizer=tf.keras.optimizers.Adam(learning_rate=0.001),\n",
    "                metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d077b5be-2127-45c5-bf57-f0bab630e08f",
   "metadata": {},
   "source": [
    "Another interesting note in the tutorial about learning rate and using multiple GPUs:\n",
    "> For this toy example with the MNIST dataset, you will be using the Adam optimizer's default learning rate of 0.001.\n",
    ">\n",
    "> For larger datasets, the key benefit of distributed training is to learn more in each training step, because each step processes more training data in parallel, which allows for a larger learning rate (within the limits of the model and dataset)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d5e693a-bafd-4056-b590-0a04d10aa00e",
   "metadata": {},
   "source": [
    "## Callbacks\n",
    "\n",
    "Callbacks are the tools we can use to monitor training, control when to stop and lots of other useful features. We'll use some handy ones here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "b3894131-d414-4f49-b015-502b5c7bdf6e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Define the checkpoint directory to store the checkpoints.\n",
    "# This setus up logging for TensorBoard\n",
    "checkpoint_dir = './training_checkpoints'\n",
    "# Define the name of the checkpoint files.\n",
    "checkpoint_prefix = os.path.join(checkpoint_dir, \"ckpt_{epoch}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "283c30b2-ebc5-445b-a1df-8d38b156e04d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Define a function for decaying the learning rate.\n",
    "# You can define any decay function you need.\n",
    "def decay(epoch):\n",
    "  if epoch < 3:\n",
    "    return 1e-3\n",
    "  elif epoch >= 3 and epoch < 7: # Drop the learning rate for epochs 3-6\n",
    "    return 1e-4\n",
    "  else:                         # Drop the learning rate again for epochs 7 and on\n",
    "    return 1e-5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "d02ffd08-0faa-41ac-98e4-d62588a06f58",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Define a callback for printing the learning rate at the end of each epoch.\n",
    "class PrintLR(tf.keras.callbacks.Callback):\n",
    "  def on_epoch_end(self, epoch, logs=None):\n",
    "    print('\\nLearning rate for epoch {} is {}'.format(        epoch + 1, model.optimizer.lr.numpy()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "98c09948-7bd3-4451-9703-394a51b9a776",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Put all the callbacks together.\n",
    "callbacks = [\n",
    "    tf.keras.callbacks.TensorBoard(log_dir='./logs'),\n",
    "    tf.keras.callbacks.ModelCheckpoint(filepath=checkpoint_prefix,\n",
    "                                       save_weights_only=True),\n",
    "    tf.keras.callbacks.LearningRateScheduler(decay),\n",
    "    PrintLR()\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "aeb449de-f690-4d28-8d67-a4b63214e784",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/12\n",
      "  1/469 [..............................] - ETA: 14s - loss: 0.0269 - accuracy: 0.9922"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-08-27 20:00:41.552129: W tensorflow/core/grappler/optimizers/data/auto_shard.cc:537] The `assert_cardinality` transformation is currently not handled by the auto-shard rewrite and will be removed.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "462/469 [============================>.] - ETA: 0s - loss: 0.0421 - accuracy: 0.9870\n",
      "Learning rate for epoch 1 is 0.0010000000474974513\n",
      "469/469 [==============================] - 1s 3ms/step - loss: 0.0423 - accuracy: 0.9869 - lr: 0.0010\n",
      "Epoch 2/12\n",
      "464/469 [============================>.] - ETA: 0s - loss: 0.0299 - accuracy: 0.9908\n",
      "Learning rate for epoch 2 is 0.0010000000474974513\n",
      "469/469 [==============================] - 1s 3ms/step - loss: 0.0298 - accuracy: 0.9908 - lr: 0.0010\n",
      "Epoch 3/12\n",
      "465/469 [============================>.] - ETA: 0s - loss: 0.0224 - accuracy: 0.9929\n",
      "Learning rate for epoch 3 is 0.0010000000474974513\n",
      "469/469 [==============================] - 1s 3ms/step - loss: 0.0223 - accuracy: 0.9929 - lr: 0.0010\n",
      "Epoch 4/12\n",
      "465/469 [============================>.] - ETA: 0s - loss: 0.0111 - accuracy: 0.9977\n",
      "Learning rate for epoch 4 is 9.999999747378752e-05\n",
      "469/469 [==============================] - 1s 3ms/step - loss: 0.0111 - accuracy: 0.9977 - lr: 1.0000e-04\n",
      "Epoch 5/12\n",
      "461/469 [============================>.] - ETA: 0s - loss: 0.0095 - accuracy: 0.9981\n",
      "Learning rate for epoch 5 is 9.999999747378752e-05\n",
      "469/469 [==============================] - 1s 3ms/step - loss: 0.0095 - accuracy: 0.9981 - lr: 1.0000e-04\n",
      "Epoch 6/12\n",
      "467/469 [============================>.] - ETA: 0s - loss: 0.0088 - accuracy: 0.9984\n",
      "Learning rate for epoch 6 is 9.999999747378752e-05\n",
      "469/469 [==============================] - 1s 3ms/step - loss: 0.0088 - accuracy: 0.9984 - lr: 1.0000e-04\n",
      "Epoch 7/12\n",
      "460/469 [============================>.] - ETA: 0s - loss: 0.0082 - accuracy: 0.9986\n",
      "Learning rate for epoch 7 is 9.999999747378752e-05\n",
      "469/469 [==============================] - 1s 3ms/step - loss: 0.0081 - accuracy: 0.9986 - lr: 1.0000e-04\n",
      "Epoch 8/12\n",
      "466/469 [============================>.] - ETA: 0s - loss: 0.0071 - accuracy: 0.9988\n",
      "Learning rate for epoch 8 is 9.999999747378752e-06\n",
      "469/469 [==============================] - 1s 3ms/step - loss: 0.0071 - accuracy: 0.9988 - lr: 1.0000e-05\n",
      "Epoch 9/12\n",
      "469/469 [==============================] - ETA: 0s - loss: 0.0070 - accuracy: 0.9989\n",
      "Learning rate for epoch 9 is 9.999999747378752e-06\n",
      "469/469 [==============================] - 1s 3ms/step - loss: 0.0070 - accuracy: 0.9989 - lr: 1.0000e-05\n",
      "Epoch 10/12\n",
      "463/469 [============================>.] - ETA: 0s - loss: 0.0070 - accuracy: 0.9989\n",
      "Learning rate for epoch 10 is 9.999999747378752e-06\n",
      "469/469 [==============================] - 1s 3ms/step - loss: 0.0069 - accuracy: 0.9989 - lr: 1.0000e-05\n",
      "Epoch 11/12\n",
      "462/469 [============================>.] - ETA: 0s - loss: 0.0069 - accuracy: 0.9990\n",
      "Learning rate for epoch 11 is 9.999999747378752e-06\n",
      "469/469 [==============================] - 1s 3ms/step - loss: 0.0069 - accuracy: 0.9990 - lr: 1.0000e-05\n",
      "Epoch 12/12\n",
      "450/469 [===========================>..] - ETA: 0s - loss: 0.0069 - accuracy: 0.9989\n",
      "Learning rate for epoch 12 is 9.999999747378752e-06\n",
      "469/469 [==============================] - 1s 3ms/step - loss: 0.0068 - accuracy: 0.9989 - lr: 1.0000e-05\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x2b8d123271f0>"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "EPOCHS = 12\n",
    "\n",
    "model.fit(train_dataset, epochs=EPOCHS, callbacks=callbacks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "9a6788e9-de56-40a3-b3e0-0f38ebdc7d63",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "checkpoint\t\t     ckpt_4.data-00000-of-00001\n",
      "ckpt_10.data-00000-of-00001  ckpt_4.index\n",
      "ckpt_10.index\t\t     ckpt_5.data-00000-of-00001\n",
      "ckpt_11.data-00000-of-00001  ckpt_5.index\n",
      "ckpt_11.index\t\t     ckpt_6.data-00000-of-00001\n",
      "ckpt_12.data-00000-of-00001  ckpt_6.index\n",
      "ckpt_12.index\t\t     ckpt_7.data-00000-of-00001\n",
      "ckpt_1.data-00000-of-00001   ckpt_7.index\n",
      "ckpt_1.index\t\t     ckpt_8.data-00000-of-00001\n",
      "ckpt_2.data-00000-of-00001   ckpt_8.index\n",
      "ckpt_2.index\t\t     ckpt_9.data-00000-of-00001\n",
      "ckpt_3.data-00000-of-00001   ckpt_9.index\n",
      "ckpt_3.index\n"
     ]
    }
   ],
   "source": [
    "# Check the checkpoint directory.\n",
    "!ls {checkpoint_dir}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "b1674a4b-21a5-4a62-9f80-19f4c9fbc037",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "24/79 [========>.....................] - ETA: 0s - loss: 0.0343 - accuracy: 0.9912 "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-08-27 20:01:18.628843: W tensorflow/core/grappler/optimizers/data/auto_shard.cc:537] The `assert_cardinality` transformation is currently not handled by the auto-shard rewrite and will be removed.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "79/79 [==============================] - 0s 2ms/step - loss: 0.0394 - accuracy: 0.9868\n",
      "Eval loss: 0.03939875587821007, Eval accuracy: 0.9868000149726868\n"
     ]
    }
   ],
   "source": [
    "model.load_weights(tf.train.latest_checkpoint(checkpoint_dir))\n",
    "\n",
    "eval_loss, eval_acc = model.evaluate(eval_dataset)\n",
    "\n",
    "print('Eval loss: {}, Eval accuracy: {}'.format(eval_loss, eval_acc))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9e64d49-fc5a-469e-8b03-a26f345b1866",
   "metadata": {},
   "source": [
    "## Look at training in TensorBoard\n",
    "\n",
    "Because of the way we are connected to the session with Jupyter, we can't easily use this session for running TensorBoard. Let's [follow the directions here to open a new HiPerGator Desktop](https://help.rc.ufl.edu/doc/Tensorboard) to look at the TensorBoard logs.\n",
    "\n",
    "* In a new tab, go to [https://ood.rc.ufl.edu/](https://ood.rc.ufl.edu/)\n",
    "* From the Interactive Apps menu, select the 1st option: HiPerGator Desktop\n",
    "* Request:\n",
    "   * 2 cores\n",
    "   * 12 GB RAM\n",
    "   * 1 hour of time\n",
    "* Click Launch\n",
    "* When the job starts, click the **Launch HiPerGator Desktop** button\n",
    "* That will open a GUI Desktop in a new session.\n",
    "* From the Applications menu, select **Terminal Emulator**\n",
    "* Change directories to the directory where this notebook is located. e.g. `cd /blue/gms6029/$USER/distributed_mnist`\n",
    "* Load the needed modules: `module load tensorflow ubuntu`\n",
    "* Launch TensorBoard: `tensorboard --logdir=./logs &` \n",
    "    * The `&` tells bash to return rather than wait for the command to complete\n",
    "* Wait a few seconds and once you see a line that says `TensorBoard 2.7.0 at http://localhost:6006/ (Press CTRL+C to quit)`, hit return to get the command prompt back\n",
    "* Type: `firefox` to open the browser\n",
    "* Go to `http://localhost:6006/`\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce432592-4abb-4cd4-9859-94a982f8efd0",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Tensorflow-2.7.0",
   "language": "python",
   "name": "tensorflow-2.7.0"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
